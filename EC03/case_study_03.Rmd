---
title: 'Estudo de Caso 03: Avaliação e comparação de duas configurações de Algoritmos Evolucionários'
author: "Joao P. L. Pinto, Thales H. de O. Gonçalves, Henrique A. Barbosa, \\
date: 23 de Dezembro de 2024"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: true
editor_options:
  markdown:
    wrap: 72
bibliography: references.bib
csl: ieee.csl
---

```{r setup,results='hide',warning=FALSE,echo=FALSE}
# A few initial definitions just to make sure all required packages are installed. Change as needed.
# NOTE: It may echo some weird messages to the PDF on the first compile (package installation messages). Run twice and the problem will (hopefully) go away.
if (!require(ggplot2, quietly = TRUE)){
      install.packages("ggplot2")
      }
if (!require(devtools, quietly = TRUE)){
      install.packages("devtools")
      }
 if (!require(broom, quietly = TRUE)){
       devtools::install_github("dgrtwo/broom")
      }
if (!require(GGally, quietly = TRUE)){
      install.packages("GGally")
}
if (!require(ggplot2, quietly = TRUE)){
      install.packages("ggplot2")
}
if (!require(reshape2, quietly = TRUE)){
      install.packages("reshape2")
}

```

## Descrição do Problema
Em diversas situações, como no design de asas de aeronaves para maximizar a eficiência aerodinâmica \cite[della2019application] ou no treinamento de robôs para executar tarefas específicas \cite[grefenstette1994evolutionary], é necessário minimizar (ou maximizar) uma função de custo de interesse. Entretanto, quando não é possível encontrar uma solução analiticamente para seu ponto de mínimo (máximo), recorre-se a métodos heurísticos de otimização. Entre esses métodos destacam-se os algoritmos evolucionários \cite[holland1992adaptation], que simulam processos naturais de evolução, utilizando populações de possíveis soluções, inicialmente geradas de forma aleatória, para evoluir gradativamente em direção a respostas que melhor atendam ao problema em questão.

Um dos algoritmos evolucionários mais comuns e amplamente utilizados é o algoritmo de Evolução Diferencial \cite[price2006differential]. Nesse método, uma população inicial de vetores-soluções é gerada aleatoriamente, e cada indivíduo da população é avaliado com base na função de custo a ser otimizada. Durante várias iterações, ou épocas, os indivíduos passam por processos de mutação e cruzamento. A mutação introduz diversidade ao alterar características de forma aleatória, enquanto o cruzamento combina as melhores soluções, promovendo o desenvolvimento da população para um melhor resultado. Ao final do processo, o indivíduo com o melhor desempenho, ou "fit", é considerado a solução do problema.

Esse algoritmo, no entanto, apresenta múltiplos métodos de recombinação e mutação, assim como diversos hiperparâmetros, que influenciam diretamente seu funcionamento, podendo resultar em variações significativas de desempenho dependendo das configurações escolhidas. Neste trabalho, busca-se comparar o desempenho de duas configurações distintas aplicadas a dois algoritmos de Evolução Diferencial, avaliando a qualidade das soluções encontradas de forma independente do número de dimensões do problema. A tabela a seguir apresenta os métodos de recombinação e cruzamento utilizados em cada configuração do algoritmo, assim como seus respectivos hiperparâmetros.

\[
\begin{table}[]
\begin{tabular}{ccc}
Configuração & Método de Recombinação                                                                                 & Método de Mutação                                                          \\
1            & Blend Alpha Beta Recombination for DE \cite[herrera2003taxonomy] com $\alpha=0$ e $\beta=0$                                 & Em indivíduos aleatórios, com fator de escala do vetor de diferenças $f=4$ \\
2            & Recombinação Exponencial \cite[price2006differential] com probabilidade de cada ponto do vetor ser um ponto de corte $cr=0,6$ & Nos melhores indivíduos, com fator de escala do vetor de diferenças $f=2$ 
\end{tabular}
\end{table}
\]

A função de custo que deseja-se minimizar no contexto desse trabalho é a função Rosembrock para os limites $-5\le x_i \le 10$, $i=1,...,dim$ para um problema de $dim \in [2,150]$ dimensões. O tamanho da população e o número máximo de iterações do algoritmo são dependentes de $dim$, sendo esses valores $5*dim$ e $100*dim$, respectivamente. Os parâmetros experimentais dados para este estudo são:

- Mínima diferença de importância prática (padronizada): $d^*=0.5$
- Significância desejada: $\alpha=0.05$
- Potência mínima desejada: $1-\beta=0.8$

Para começar o código, primeiro definimos funções capazes de gerar os problemas e definir as configurações a serem utilizadas:
```{r loaddata4}
dimensao <- 10
fn <- function(X){
  if(!is.matrix(X)) X <- matrix(X, nrow = 1) # <- if a single vector is passed as X
  Y <- apply(X, MARGIN = 1,
             FUN = smoof::makeRosenbrockFunction(dimensions = dimensao))
  return(Y)
}
# testing the function on a matrix composed of 2 points
X <- matrix(runif(20), nrow = 2) # runif gera uma distribuição uniforme aleatória
fn(X)

configuracoes <- list(
  list(name = "Config 1", recpars = list(name = "recombination_blxAlphaBeta", alpha = 0, beta = 0), mutpars = list(name = "mutation_rand", f = 4)),
  list(name = "Config 2", recpars = list(name = "recombination_exp", cr = 0.6), mutpars = list(name = "mutation_best", f = 2))
)


```



# Design dos Experimentos

Conforme descrito na seção anterior, este estudo tem como objetivo comparar o desempenho de duas configurações distintas do algoritmo de evolução diferencial no contexto de minimização da função Rosenbrock, considerando problemas com dimensões variando de 2 a 150. Para isso, busca-se:

- Verificar se existe uma diferença estatisticamente significativa no desempenho médio entre as duas configurações avaliadas; e
- Identificar qual das configurações apresenta o melhor desempenho, caso uma diferença seja constatada.

A hipótese nula estabelecida neste estudo é que os dois níveis comparados não apresentam diferenças significativas na média de seus desempenhos. Essa hipótese será rejeitada caso seja identificada alguma diferença estatisticamente significativa. No entanto, o desempenho dos algoritmos varia em função da dimensão do problema, um efeito indesejado para a comparação dos níveis. Essa variabilidade dificulta assumir que as condições experimentais sejam homogêneas, tornando inadequado o uso do teste ANOVA simples para verificar diferenças na média entre os níveis comparados.

Para isolar o efeito da dimensão do problema e tornar o experimento mais robusto e generalizável, emprega-se uma estratégia de blocagem com base no número de dimensões do problema. Nesse contexto, cada bloco corresponde a uma dimensão dentro do intervalo [2,150]. Formalmente, o modelo estatístico é definido como:

$$y_{ij}=\mu+\tau_i+\beta_j+\epsilon_{ij}\begin{cases}i=1,...,a &\\j=1,...,b\end{cases}$$
onde $y_{ij}$ é uma observação da amostra, $\mu$ é a média da amostra, $\tau_i$ o tamanho do efeito devido aos níveis, $\beta_j$ o efeito dos blocos, $\epsilon_{ij}$ o resíduo, e $a$ e $b$ o número de níveis e o número de blocos, respectivamente. No teste, o interesse está exclusivamente no fator experimental referente aos níveis, de modo que a hipótese é formulada da seguinte forma:

$$\begin{cases} H_0: \tau_{i} = 0, \forall i=1,...,a&\\H_1: \exists \tau_{i}\neq0\end{cases}$$

Para isso, é necessário determinar o número de dimensões a serem blocadas e a quantidade de amostras por blocos para se obter os parâmetros experimentais desejados, especificados na seção anterior.

# Quantidade de Dimensões

Apesar de existirem 148 grupos presentes no problema, considerando cada grupo como uma dimensão de resolução do algoritmo, não há uma necessidade efetiva de serem testados todos os grupos de variações. Para isso, podemos definir o poder do teste, assim como o número de grupos presentes no experimento, de forma a descobrir o número de instâncias necessárias.

No caso, temos que o número de grupos condiz com a quantidade de configurações do algoritmo, ou seja, duas. Além disto, seguindo o padrão expresso na literatura, temos que $\beta = 0.8$. O tamanho de efeito mínimo relevante foi definido como 0.5, uma proposta baseada no decaimento lento da função de Rosenbrock, conseguindo detectar dificuldades de caminhar em bacias de atração com gradientes pouco significativos.

O código utilizado foi baseado na biblioteca CAISEr, tal como apresentado em [@campello2019sample] e [@campelo2020sample].

## ALTERAR O CÓDIGO ABAIXO DE FORMA A EXTRUTURAR NO CONTEXTO GERAL
```{r loaddata4}
dimensoes = calc_instances(2, 0.5, power=0.8)$ninstances
```


# Definição de quantidade de amostras
O planejamento da quantidade de amostras de resultado para cada dimensão é um passo importante para alcançar melhores
resultados na análise de experimentos, uma vez que com uma quantidade insuficiente de amostras corremos o risco de observar o efeito da variação das amostras se sobrepondo a variação das dimensões e das configuraçãoes dos algoritmos. Conforme proposto por [@campelo2019sample] implementamos um algoritmo iterativo para definir o tamanho de amostras necessária para cada dimensão do problema. O processo se deu da seguinte forma:

* Calculamos a quantidade de dimensões necessárias ($n_{blocos}$) para que o teste tenha poder estatístico de 80%
* Selecionamos a quantidade $n_{blocos}$ de dimensões igualmente espaçadas entre sí partindo de 2 a 150 dimensões.

Depois de definir as dimensões, partimos para a definição da quantidade de amostras por dimensão:
* Definimos o desvio padrão máximo desejado ($se*$): 
  * selecionamos 10 dimensões aleatórias;
  * executamos, para cada dimensão de teste e para cada configuração, 10 repetições do algoritmo ExpDE e armazenamos os resultados. 
  * Calculamos o desvio padrão dos resultados e calculamos o coeficiente de variaçãod e cada dimensão ($CV = \frac{\sigma}{\mu}$) e selecionamos o desvio padrão da amostra que obteve o menor coeficiente de variação. No caso, $se*= 22838.19$
* Selecionamos 10 dimensões aleatórias e calculamos a quantidade de iterações necessária para que os resultados obtidos para essa dimensão tenham no máximo o desvio padrão especificado $se*$ especificado. A quantidade mínima definida foi de 2 iterações e a máxima de 50 iterações, considerando a inviabilidade de executar mais de 50 iterações em cada dimensão e iteração.
* Após obter a quantidade de iterações de cada dimensão, selecionamos a maior quantidade de repetições e definimos como a quantidade de todas a dimensões, com o objetivo de garantir que no pior caso ainda teremos um bom nível de confiança e poder do teste, e conseguentemente para todas as outras dimensões. A quantidade de repetições definida foi de 48 repetições por dimensão-configuração.

O algoritmo implmentado é apresentado em [@campelo2019sample] e implementado em um script R separado (EC3_expecificacao_tamanho_amostras.R)

A quantidade de repetições pode ser, assim, definida a partir do seguinte código:

```{r loaddata4}
sample_iterations <- function(instance, algo1, algo2, se_threshold, n0, n_max) {
  # Inicializando as amostras iniciais
  x1 <- sample(algo1(instance), n0, replace = TRUE)
  x2 <- sample(algo2(instance), n0, replace = TRUE)
  
  # Número inicial de execuções
  n1 <- n0
  n2 <- n0
  
  # Função para calcular erro padrão da diferença simples
  calc_se <- function(x1, x2) {
    s1 <- sd(x1)
    s2 <- sd(x2)
    sqrt(s1^2 / length(x1) + s2^2 / length(x2))
  }
  
  # Calcula o erro padrão inicial
  se_current <- calc_se(x1, x2)
  
  # Loop até atingir o limite de precisão ou o orçamento máximo
  while (se_current > se_threshold && (n1 + n2) < n_max) {
    # Calcula a proporção ótima de amostras
    s1 <- sd(x1)
    s2 <- sd(x2)
    r_opt <- s1 / s2
    
    # Determina qual algoritmo amostrar com base na proporção
    if (n1 / n2 < r_opt) {
      # Adiciona uma nova amostra para o algoritmo 1
      new_sample <- algo1(instance)
      x1 <- c(x1, new_sample)
      n1 <- n1 + 1
    } else {
      # Adiciona uma nova amostra para o algoritmo 2
      new_sample <- algo2(instance)
      x2 <- c(x2, new_sample)
      n2 <- n2 + 1
    }
    
    # Atualiza o erro padrão
    se_current <- calc_se(x1, x2)
  }
  
  # Retorna os resultados
  list(
    x1 = x1,
    x2 = x2,
    n1 = n1,
    n2 = n2,
    se = se_current
  )
}

```

Criando wrappers para realizar as amostragens, temos o seguinte código:

```{r loaddata4}
n_amostrar_para_erro = 10
n_dimensoes_para_erro = 10

# estimativa de amostras
# se_threshold, 
n0=2
n_max=50
amostras_teste=10
se_threshold = 22838.192087

wrapper_config1 <- function(dimensao){
  selpars <- list(name = "selection_standard")
  stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimensao, maxiter = 100 * dimensao)
  probpars <- list(name = "fn", xmin = rep(-5, dimensao), xmax = rep(10, dimensao))
  popsize = 5 * dimensao
  out <- ExpDE(mutpars = configuracoes[[1]]$mutpars,
               recpars = configuracoes[[1]]$recpars,
               popsize = popsize,
               selpars = selpars,
               stopcrit = stopcrit,
               probpars = probpars,
               showpars = list(show.iters = "dots", showevery = 20))
  return(out$Fbest)
}
wrapper_config2 <- function(dimensao){
  selpars <- list(name = "selection_standard")
  stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimensao, maxiter = 100 * dimensao)
  probpars <- list(name = "fn", xmin = rep(-5, dimensao), xmax = rep(10, dimensao))
  popsize = 5 * dimensao
  out <- ExpDE(mutpars = configuracoes[[2]]$mutpars,
               recpars = configuracoes[[2]]$recpars,
               popsize = popsize,
               selpars = selpars,
               stopcrit = stopcrit,
               probpars = probpars,
               showpars = list(show.iters = "dots", showevery = 20))
  return(out$Fbest)
}



dimensoes_teste = sample(dimensoes, amostras_teste)
```

Por fim, definimos o número de repetições a seguir:
```{r loaddata4}
library("CAISEr")
# Defina os parâmetros
# dimensaoensoes, configuracoes e repeticoes devem ser definidos como vetores ou listas.
# dimensoes<- c(10, 20)  # Exemplos de dimensaoensões
repeticoes <- 1:n_amostrar_para_erro  # Número de repetições

n_blocos = calc_instances(2, 0.5, power=0.8) # 2: n instancias, 0.5: tamanho de efeito mínimo; power: poder do teste
dimensoes = seq(2,150,length.out=38)
dimensoes_teste = sample(dimensoes, 10)  # amostragem para calculo de quantidade de repetições
tamanho_amostras <- data.frame(
  Dimensao = numeric(),    # Coluna tipo numérico
  Iterations1 = numeric(), # Evita fatores (para colunas de texto)
  Iterations2 = numeric() # Evita fatores (para colunas de texto)
)
for (dimensao in dimensoes_teste){
  cat("Dimensao: ", dimensao)
  si = sample_iterations(
    instance = dimensao,
    algo1 = wrapper_config1,
    algo2 = wrapper_config2,
    se_threshold = se_threshold,
    n0 = n0,
    n_max = n_max
  )
  cat("\nAmostra 1: ", si$n1,"| Amostra 2: ", si$n2, "\n")
  tamanho_amostras[nrow(tamanho_amostras)+1,] <- list(dimensao, si$n1, si$n2)
  
}
  

# Parâmetros adicionais para o ExpDE
resultados <- data.frame(matrix(ncol = length(configuracoes), nrow = length(dimensoes)))
repeticoes_resultados <- data.frame(
  Config = character(),   # Coluna tipo texto
  Dimensao = numeric(),    # Coluna tipo numérico
  Repeticao = numeric(),    # Coluna tipo double
  Valor = double() # Evita fatores (para colunas de texto)
)
colnames(resultados) <- sapply(configuracoes, function(config) config$name)
rownames(resultados) <- dimensoes
repeticoes <- 1:48  # Número de repetições

# Loop sobre dimensaoensões, configurações e repetições
for (dimensao in dimensoes) {
  for (configuracao in configuracoes) {
    resultados_config <- c()  # Vetor para armazenar os resultados das repetições
    i = 0
    for (repeticao in repeticoes) {
      selpars <- list(name = "selection_standard")
      stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimensao, maxiter = 100 * dimensao)
      probpars <- list(name = "fn", xmin = rep(-5, dimensao), xmax = rep(10, dimensao))
      popsize = 5 * dimensao
      # Executa o algoritmo ExpDE e armazena o resultado
      # Run algorithm on problem:
      out <- ExpDE(mutpars = configuracao$mutpars,
                   recpars = configuracao$recpars,
                   popsize = popsize,
                   selpars = selpars,
                   stopcrit = stopcrit,
                   probpars = probpars,
                   showpars = list(show.iters = "dots", showevery = 20))
      # Extract observation:
      resultados_config <- c(resultados_config, out$Fbest)  # Supondo que "fitness" seja uma métrica retornada
      cat("Configuracao", configuracao$name, "| dimensão: ", dimensao, "| resultado: ", resultados_config)
      repeticoes_resultados[nrow(repeticoes_resultados)+1,] <- list(configuracao$name, dimensao, i, out$Fbest)
      i = i+1
      
      #print(resultados_config)
    }
    resultados[as.character(dimensao), configuracao$name] <- mean(resultados_config)
  }
}

write.csv(resultados, "resultados_comparacao_algoritmo.csv")
write.csv(repeticoes_resultados, "resultado_cada_repeticao.csv")

novo_df <- data.frame(
  Dimensão = rep(rownames(resultados), times = ncol(resultados)),
  Configuração = rep(colnames(resultados), each = nrow(resultados)),
  Resultado = as.vector(as.matrix(resultados))
)
write.csv(novo_df, "resultados_comparacao_algoritmo_REORGANIZADO.csv")
```

# Teste de Blocagem

O teste de blocagem, como explicado anteriormente, permite avaliar a significância de parâmetros pressupondo outros como decorrentes de variações normalmente distribuídas. Uma vez que em R a função de teste ANOVA é capaz de separar o experimento por grupos, iremos utilizar ela de forma a 

## Número da Amostra

Apesar de não ser possível conseguir novos dados para a execução dos
testes, é relevante confirmar se a quantidade de dados aqui presentes consegue
gerar o poder do teste esperado para nosso experimentos. Vale notar que a relevância do fator $\delta$ foi tomado como o desvio padrão dos valores, sendo o fator de relevância mais natural considerando o problema em questão.

Valores de $\alpha$ e $\beta$ utilizados são valores padrões na literatura, sendo valores de significância estatística consideráveis. O parâmetro $a$ refere-se ao número de grupos em questão, ou seja, 5 conjuntos de ações.

No contexto deste estudo, assume-se que apenas dois grupos apresentam desvios em relação à média global, enquanto os demais têm valores nulos de $\tau$. Essa abordagem reflete uma situação onde as diferenças significativas estão concentradas em apenas dois níveis.

O vetor $\tau$ foi construído de forma a representar essa suposição, com dois valores simetricamente opostos ($-\delta / 2$ e $\delta / 2$), enquanto os demais níveis são definidos como 0. A variância entre os grupos é então calculada com base nesse vetor, sendo um parâmetro fundamental para estimar o número de amostras necessárias para alcançar o poder estatístico desejado.

O código que segue traduz essa abordagem:

```{r loaddata3}

```

O valor de n presente no teste é inferior ao número de dados que possuímos para cada
grupo, com isso podemos assegurar um poder estatístico de 80% de não cometer erros do tipo II (Falso negativo).

## Teste ANOVA - ATUALIZAR!!!

Podemos realizar o teste utilizando pacotes próprios do R da seguinte forma:

```{r loaddata4}

```

O P valor resultante é extremamente baixo e abaixo do $\alpha$ definido, ou seja, podemos rejeitar a hipótese nula e concluir que de fato existem ações melhores que outras.

## Premissas do Teste

Dotados dos resíduos do teste, podemos conferir as premissas específicas do próprio teste. Inicialmente, verificamos se os resíduos seguem de fato uma normal padrão

```{r loaddata5}
# Check normality
#shapiro.test(model$residuals)

```

```{r plotnormaldata}

```

```{r loaddata6}
#qqnorm(model$residuals)
#qqline(model$residuals)
```

Podemos concluir que os resíduos de fato seguem suficientemente bem uma distribuição normal padrão, com um certo comportamento divergente nas caudas. Por fim, precisamos checar a igualdade de variâncias entre cada um dos fatores, ou seja, sua homoscedasticidade.

```{r loaddata7}
# Check homoscedasticity
#fligner.test(novo_df$Resultado ~ novo_df$Configuração, 
#             data = novo_df)
```
Novamente podemos confirmar a premissa do teste, sendo que o ANOVA com blocos se apresenta realmente como um teste confiável para este problema.

# Comparação Entre os Níveis

Após a realização do teste aleatorizado com blocos, e a rejeição de $H_0$, conclui-se que os níveis são significativamente diferentes entre si, independentemente do número de dimensões do problema. Isso indica que uma configuração de algoritmo apresenta desempenho superior ao outro.

Para um experimento com apenas dois níveis, o ideal é executar um teste t pareado. No entanto, como o problema em questão é um caso particular de uma comparação múltipla com apenas dois níveis, para identificar qual configuração oferece o melhor desempenho, decidiu-se por aplicar um teste post-hoc de Dunnett de todos os grupos contra um. Esse teste, na prática, equivale a um teste t quando se trata de apenas dois níveis. Por fim, o intervalo de confiança da diferença entre os níveis é exibido para facilitar a interpretação dos resultados.

```{r Comparison}

# Situation: all vs. one
#library(multcomp)
## comparação da configuraçao #######
#duntest     <- glht(model,
                  #  linfct = mcp(Configuração = "Dunnett"))

#summary(duntest)

#duntestCI   <- confint(duntest)

#par(mar = c(5, 8, 4, 2), las = 1)
#plot(duntestCI,
    # xlab = "Mean difference")
#
## comparação da configuração #######
#duntest     <- glht(model2,
           #         linfct = mcp(Dimensao = "Dunnett"))

#summary(duntest)

#duntestCI   <- confint(duntest)

#par(mar = c(5, 8, 4, 2), las = 1)
#plot(duntestCI,
    # xlab = "Mean difference")
```

É claramente perceptível que a diferença média de desempenho entre as configurações 2 e 1 é negativa em todo o intervalo de confiança, indicando que a configuração 2 sobressai em relação à configuração 1. Com base nisso, a configuração 2 foi selecionada como a de melhor desempenho.


# Discussões e Conclusões

A análise comparativa do retorno médio das ações revelou insights valiosos sobre o desempenho relativo dos ativos avaliados. O uso robusto de métodos estatísticos, como o teste ANOVA e o teste de Tukey, permitiu identificar diferenças significativas entre os grupos. No entanto, as limitações observadas, como a suposição de homoscedasticidade e a falta de variáveis macroeconômicas, ressaltam a necessidade de uma análise mais abrangente em estudos futuros para capturar a complexidade do mercado.

Concluímos que a metodologia aplicada não só demonstrou sua eficácia na validação estatística das diferenças de desempenho, mas também poderá servir como uma estrutura útil para avaliações de ativos em diversos contextos. À medida que investidores e analistas buscam otimizar suas decisões, a incorporação de análises mais profundas, que considerem fatores externos, poderá enriquecer ainda mais as estratégias de investimento. Portanto, encorajamos novas pesquisas que aprofundem essa discussão, contribuindo para um entendimento mais sólido das dinâmicas do mercado financeiro.

# Bibliografia

